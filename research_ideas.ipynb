{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Based Output Feedback Synthesis\n",
    "\n",
    "This document summarizes the various discussions among Lucas, Pete and Russ regarding output feedback controller synthesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question/Goal\n",
    "\n",
    "- Use model based techniques to synthesize output feedback controllers.\n",
    "- Offer an alternative to the end-to-end deep learning approaches?\n",
    "- Simple/interpretable controllers?\n",
    "- Controllers with performance/stability guarantees?\n",
    "\n",
    "# Technical Approaches\n",
    "\n",
    "Here we list the different technical approaches we have discussed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization over output feedback policies\n",
    "\n",
    "We would like to directly optimize an output feedback policy $\\pi$, i.e.\n",
    "\n",
    "\\begin{align*}\n",
    "        \\min_{x(.), u(.), \\pi} \\ \\ \\ & \\int_{t_0}^{t_f} g(x(t),u(t)) dt\\\\\n",
    "        s.t. \\ \\ \\  & \\forall t, \\ \\ \\dot{x}(t) = f(x(t),u(t)), \\\\\n",
    "        & x(t_0) = x_0, \\\\\n",
    "        & u = \\pi(y(x))\n",
    "\\end{align*}\n",
    "\n",
    "Where $y(x)$ is the renderer.\n",
    "\n",
    "We would like to solve this optimization from many initial samples $x_{0,i}$ and simultaneously optimize the feedback policy given a particular form, for example linear feedback $u = Ky(x)$.\n",
    "\n",
    "When we have many initial samples the objective could be replaced with\n",
    "\n",
    "\\begin{equation*}\n",
    " \\min_{x(.), u(.), \\pi} \\sum_i \\int_{t_0}^{t_f} g(x_i(t),u_i(t)) dt\\\\\n",
    "\\end{equation*}\n",
    "\n",
    "The key is that the same control policy $u = \\pi(y)$ is used for all samples.\n",
    "\n",
    "### Technical Details\n",
    "\n",
    "- This is really o-policy learning\n",
    "- Use GPU + PyTorch to do forward simulation in \"batch\". i.e. forward simulate from many individual initial conditions. Use this batch simulation to take a gradient step.\n",
    "- Is there a better way to do \n",
    "\n",
    "\n",
    "### Questions\n",
    "\n",
    "- How does this relate to standard reinforcement learning?\n",
    "- What optimization techniques should we use to tackle this problem? SNOPT, PyTorch etc.\n",
    "\n",
    "### Examples\n",
    "\n",
    "- Double integrator stabilization\n",
    "- Pendulum swing up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simultaneous search for policy and verifying Lyapunov function\n",
    "\n",
    "### Idea\n",
    "Sampling based approach of LQR trees and SOS funnels.\n",
    "\n",
    "$$\\dot{x} = f(x,u)$$\n",
    "\n",
    "$$ u = \\pi_\\theta(x)) $$\n",
    "\n",
    "$$ V(x) = p.s.d. \\text{by construction, but parameterized by parameters } \\psi $$\n",
    "\n",
    "$$ \\dot{V} = \\frac{dV}{dx} \\dot{x}$$\n",
    "$$ = \\big[ \\frac{dV}{dx}\\big]^T \\big[f(x,\\pi_{\\theta}(x) \\big] $$\n",
    "\n",
    "### Loss function\n",
    "\n",
    "$X$ = {$x_1, x_2, ..., x_N$} many samples\n",
    "\n",
    "$$ L(\\theta) = \\sum_{i} l(x_i, \\theta) $$\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathcal{l}(x_i,\\theta) =\n",
    "  \\begin{cases}\n",
    "    \\dot{V}(x_i, \\theta) & \\text{if $\\dot{V}(x_i, \\theta) > 0$} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Questions\n",
    "\n",
    "- Is enforcing that $V$ be p.s.d by construction too restrictive? i.e. can we get function approximators that are sufficiently rich without using something like a deep neural network?\n",
    "\n",
    "- What are the advantages/disadvantages of this method compared to the direct policy search?\n",
    "    - No longer optimal control since we have no objective function. It's really about stability more than optimiality.\n",
    "    - Optimizing two things simultaneously (Lyapunov function and policy), does this make the optimization harder and/or more unstable?\n",
    "\n",
    "### Examples\n",
    "- Pendulum swing up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Work\n",
    "\n",
    "- How does this compare to standard RL?\n",
    "    - Sergey Levine Guided Policy Search\n",
    "    - Chelsea Finn PR2 Robot w/ spatula etc.\n",
    "- What are we solving that isn't solved by LQR trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
